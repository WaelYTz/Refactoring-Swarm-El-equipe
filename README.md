# ğŸ¤– The Refactoring Swarm

> **Multi-Agent AI System for Automated Python Code Refactoring**  
> IGL Lab 2025-2026 â€” ESI Algiers

A collaborative multi-agent system that automatically detects, fixes, and validates Python code quality issues using Google Gemini LLM, LangChain, and LangGraph. Three specialized AI agents work together in a self-healing pipeline to transform buggy code into clean, tested code.

---

## ğŸ“‹ Table of Contents

- [Overview](#overview)
- [Architecture](#architecture)
- [Agents](#agents)
- [Project Structure](#project-structure)
- [Setup & Installation](#setup--installation)
- [Usage](#usage)
- [Testing](#testing)
- [Telemetry & Monitoring](#telemetry--monitoring)
- [Team Roles](#team-roles)
- [License](#license)

---

## Overview

The Refactoring Swarm implements a **relay-style agent pipeline** where each agent has a specialized role:

1. **Listener (Auditor)** â€” Analyzes code and detects issues using Pylint + LLM
2. **Corrector (Fixer)** â€” Applies intelligent fixes using Gemini LLM
3. **Validator (Judge)** â€” Generates and runs tests to verify correctness

If tests fail, a **self-healing loop** sends error feedback back to the Corrector for retry, up to a configurable maximum number of iterations.

---

## Architecture

```
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚     START       â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
                             â–¼
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚    LISTENER     â”‚ â† Analyzes code, detects issues
                    â”‚   (Auditor)     â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
                             â–¼
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â”Œâ”€â”€â”€â”€â”€â”‚   DECISION      â”‚â”€â”€â”€â”€â”€â”
              â”‚     â”‚    NODE         â”‚     â”‚
              â”‚     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚
              â”‚                             â”‚
        issues_found              no_issues_found
              â”‚                             â”‚
              â–¼                             â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚   CORRECTOR     â”‚           â”‚      END        â”‚
    â”‚    (Fixer)      â”‚           â”‚   (Success)     â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚
             â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚   VALIDATOR     â”‚ â† Generates & runs tests
    â”‚    (Judge)      â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚
        â”Œâ”€â”€â”€â”€â”´â”€â”€â”€â”€â”
        â”‚         â”‚
   tests_pass  tests_fail
        â”‚         â”‚
        â–¼         â–¼
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚  END   â”‚  â”‚  SELF-HEALING LOOP   â”‚
   â”‚SUCCESS â”‚  â”‚  â†’ Back to CORRECTOR â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

The system supports two orchestration modes:
- **LangGraph** (default) â€” Uses a compiled state graph with conditional edges
- **Legacy** â€” Simple relay-based orchestrator with manual state machine

---

## Agents

| Agent | Role | Description |
|-------|------|-------------|
| **ListenerAgent** | Auditor ğŸ” | Runs Pylint static analysis + LLM-powered issue detection on target Python files |
| **CorrectorAgent** | Fixer ğŸ”§ | Receives issue reports and generates corrected code using Gemini LLM |
| **ValidatorAgent** | Judge âš–ï¸ | Generates semantic unit tests with LLM, runs them with pytest, triggers self-healing on failure |

All agents inherit from `BaseAgent` and implement the `run(context) â†’ context` interface.

---

## Project Structure

```
Refactoring-Swarm-El-equipe/
â”œâ”€â”€ main.py                        # Entry point, orchestrator, CLI, state machine
â”œâ”€â”€ requirements.txt               # Python dependencies
â”œâ”€â”€ check_setup.py                 # Environment verification script
â”œâ”€â”€ .env.example                   # Template for API key configuration
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ agents/                    # AI Agent implementations
â”‚   â”‚   â”œâ”€â”€ base_agent.py          #   Abstract base class (BaseAgent)
â”‚   â”‚   â”œâ”€â”€ listener_agent.py      #   Auditor: code analysis & issue detection
â”‚   â”‚   â”œâ”€â”€ corrector_agent.py     #   Fixer: LLM-powered code correction
â”‚   â”‚   â””â”€â”€ validator_agent.py     #   Judge: test generation & validation
â”‚   â”‚
â”‚   â”œâ”€â”€ graph/                     # LangGraph execution graph
â”‚   â”‚   â””â”€â”€ execution_graph.py     #   State graph with nodes & conditional edges
â”‚   â”‚
â”‚   â”œâ”€â”€ prompts/                   # Prompt engineering & templates
â”‚   â”‚   â”œâ”€â”€ listener_prompts.py    #   Auditor analysis prompts
â”‚   â”‚   â”œâ”€â”€ corrector_prompts.py   #   Fixer correction prompts
â”‚   â”‚   â”œâ”€â”€ validator_prompts.py   #   Judge test generation prompts
â”‚   â”‚   â””â”€â”€ context_manager.py     #   Token optimization & context preparation
â”‚   â”‚
â”‚   â”œâ”€â”€ tools/                     # Shared tooling for agents
â”‚   â”‚   â”œâ”€â”€ sandbox.py             #   Security: path validation & sandboxing
â”‚   â”‚   â”œâ”€â”€ file_operations.py     #   Safe read/write/delete with security
â”‚   â”‚   â”œâ”€â”€ code_analyzer.py       #   Pylint wrapper for static analysis
â”‚   â”‚   â””â”€â”€ test_runner.py         #   Pytest wrapper for test execution
â”‚   â”‚
â”‚   â””â”€â”€ utils/                     # Utilities
â”‚       â”œâ”€â”€ logger.py              #   Experiment logging (JSON telemetry)
â”‚       â””â”€â”€ telemetry_dashboard.py #   Dashboard & HTML report generation
â”‚
â”œâ”€â”€ tests/                         # Test suite
â”‚   â”œâ”€â”€ test_integration.py        #   End-to-end integration tests
â”‚   â”œâ”€â”€ test_logger_quick.py       #   Logger validation tests
â”‚   â””â”€â”€ fixtures/                  #   Test data
â”‚       â”œâ”€â”€ buggy_code/            #     Intentionally buggy Python files
â”‚       â”‚   â”œâ”€â”€ calculator.py
â”‚       â”‚   â”œâ”€â”€ data_processor.py
â”‚       â”‚   â””â”€â”€ string_utils.py
â”‚       â””â”€â”€ expected_fixes/        #     Reference correct implementations
â”‚           â”œâ”€â”€ calculator.py
â”‚           â”œâ”€â”€ data_processor.py
â”‚           â””â”€â”€ string_utils.py
â”‚
â”œâ”€â”€ logs/
â”‚   â””â”€â”€ experiment_data.json       # Telemetry log (auto-generated)
â”‚
â””â”€â”€ telemetry_report.html          # HTML telemetry report (auto-generated)
```

---

## Setup & Installation

### Prerequisites

- **Python 3.10 or 3.11**
- **Google Gemini API Key** ([Get one here](https://aistudio.google.com/apikey))

### 1. Clone the Repository

```bash
git clone https://github.com/WaelYTz/Refactoring-Swarm-El-equipe.git
cd Refactoring-Swarm-El-equipe
```

### 2. Create a Virtual Environment

```bash
python -m venv venv

# Windows
venv\Scripts\activate

# Linux/macOS
source venv/bin/activate
```

### 3. Install Dependencies

```bash
pip install -r requirements.txt
```

### 4. Configure API Key

```bash
cp .env.example .env
```

Edit `.env` and add your Google Gemini API key:

```env
GOOGLE_API_KEY=AIzaSy...your-key-here
```

### 5. Verify Setup

```bash
python check_setup.py
```

---

## Usage

### Basic Run

```bash
# Refactor code in a target directory
python main.py --target_dir ./path/to/your/code
```

### Advanced Options

```bash
# With custom iteration limit and verbose output
python main.py --target_dir ./my_project --max_iterations 5 --verbose

# Dry run (analysis only, no changes)
python main.py --target_dir ./my_project --dry_run

# Show execution graph visualization
python main.py --show-graph

# Use legacy orchestrator instead of LangGraph
python main.py --target_dir ./my_project --use-legacy
```

### Quick Test with Fixtures

```bash
# Copy buggy code to a sandbox directory
mkdir -p sandbox/test_run
cp tests/fixtures/buggy_code/*.py sandbox/test_run/

# Run the swarm
python main.py --target_dir ./sandbox/test_run --verbose
```

### CLI Arguments

| Argument | Description | Default |
|----------|-------------|---------|
| `--target_dir` | Directory containing Python files to refactor | *(required)* |
| `--max_iterations` | Max self-healing loop iterations | `10` |
| `--verbose` | Enable detailed output | `True` |
| `--dry_run` | Analysis only, no code modifications | `False` |
| `--show-graph` | Display the execution graph and exit | â€” |
| `--use-legacy` | Use legacy relay orchestrator | `False` |

---

## Testing

### Run Integration Tests

```bash
pytest tests/ -v
```

### Run Specific Test Suites

```bash
# Integration tests
pytest tests/test_integration.py -v

# Logger validation tests
pytest tests/test_logger_quick.py -v
```

The test suite validates:
- Fixture file existence and structure
- Buggy code quality scores (confirming issues exist)
- Experiment log format (`experiment_data.json`)
- Action type enum compliance
- LLM prompt logging requirements

---

## Telemetry & Monitoring

All agent interactions are logged to `logs/experiment_data.json` for scientific analysis and grading.

### View Dashboard

```bash
python src/utils/telemetry_dashboard.py
```

### Export HTML Report

```bash
python src/utils/telemetry_dashboard.py --export telemetry_report.html
```

### Log Entry Schema

Each log entry contains:

| Field | Description |
|-------|-------------|
| `id` | Unique entry identifier |
| `timestamp` | ISO 8601 timestamp |
| `agent` | Agent name (e.g., `Auditor_Agent`) |
| `model` | LLM model used (e.g., `gemini-2.5-flash`) |
| `action` | Action type enum (`ANALYSIS`, `FIX`, `CODE_GEN`, etc.) |
| `details` | Action-specific payload |
| `status` | `SUCCESS` or `FAILURE` |
| `input_prompt` | Full prompt sent to LLM |
| `output_response` | Full LLM response |

---

## Team Roles

| Role | Responsibility |
|------|----------------|
| **Lead Dev (Orchestrateur)** | Execution graph design, relay handover logic, CLI, state machine |
| **Auditor Agent Dev** | Listener agent â€” Pylint + LLM code analysis |
| **Fixer Agent Dev** | Corrector agent â€” LLM-powered code correction |
| **Judge Agent Dev** | Validator agent â€” semantic test generation & self-healing loop |
| **Toolsmith** | Sandbox security, file operations, Pylint/pytest wrappers |
| **Prompt Engineer** | System prompts, templates, context optimization, prompt versioning |
| **Data Officer** | Logger, telemetry dashboard, test fixtures, integration tests |

---

## Tech Stack

- **LLM**: Google Gemini 2.5 Flash (via `langchain-google-genai`)
- **Orchestration**: LangGraph (state graph with conditional edges)
- **Framework**: LangChain (LLM integration layer)
- **Static Analysis**: Pylint
- **Testing**: Pytest
- **Language**: Python 3.10/3.11

---

## License

This project is developed as part of the **IGL Lab 2025-2026** course at **ESI Algiers** (Ã‰cole nationale SupÃ©rieure d'Informatique).

---

*Built with ğŸ¤– by El Equipe â€” ESI Algiers, IGL Lab 2025-2026*